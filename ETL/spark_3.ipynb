{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import init_spark_env\n",
    "import pyspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, avg, stddev, stddev_pop, isnan, lower, transform, udf, replace, monotonically_increasing_id\n",
    "from pyspark.sql.types import IntegerType, StructField, StructType, DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local').enableHiveSupport().appName('test_cli').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.141.211:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test_cli</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd924e7ded0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS test.advertiser;\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS test.sales;\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS test.company;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 15:29:30 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "24/12/12 15:29:30 WARN HiveMetaStore: Location: file:/home/alexandr/stydy/eda/sql_git/SQL/ETL/spark-warehouse/test.db/sales specified for non-external table:sales\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          CREATE TABLE IF NOT EXISTS test.sales (\n",
    "          `SalesID` Int,\n",
    "          `advertise_id` Int,\n",
    "          `compaign` Int);\n",
    "          ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('INSERT INTO test.sales values(1, 1, 1);')\n",
    "spark.sql('INSERT INTO test.sales values(2, 2, 2);')\n",
    "spark.sql('INSERT INTO test.sales values(3, 3, 3);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('INSERT INTO test.sales values(4, 1, 1);')\n",
    "spark.sql('INSERT INTO test.sales values(5, 2, 2);')\n",
    "spark.sql('INSERT INTO test.sales values(6, 3, 3);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('INSERT INTO test.sales values(7, 1, 1);')\n",
    "spark.sql('INSERT INTO test.sales values(8, 1, 1);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 15:29:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "24/12/12 15:29:32 WARN HiveMetaStore: Location: file:/home/alexandr/stydy/eda/sql_git/SQL/ETL/spark-warehouse/test.db/advertiser specified for non-external table:advertiser\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS test.advertiser (AdvertiserID Int,name String);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('INSERT INTO test.advertiser values(1, \"advertiser_1\");')\n",
    "spark.sql('INSERT INTO test.advertiser values(2, \"advertiser_2\");')\n",
    "spark.sql('INSERT INTO test.advertiser values(3, \"advertiser_3\");')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 15:29:33 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "24/12/12 15:29:33 WARN HiveMetaStore: Location: file:/home/alexandr/stydy/eda/sql_git/SQL/ETL/spark-warehouse/test.db/company specified for non-external table:company\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS test.company (CompanyID Int,compaing_name String);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('INSERT INTO test.company values(1, \"company_1\");')\n",
    "spark.sql('INSERT INTO test.company values(2, \"company_2\");')\n",
    "spark.sql('INSERT INTO test.company values(3, \"company_3\");')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+\n",
      "|SalesID|advertise_id|compaign|\n",
      "+-------+------------+--------+\n",
      "|      1|           1|       1|\n",
      "|      2|           2|       2|\n",
      "|      8|           1|       1|\n",
      "|      6|           3|       3|\n",
      "|      7|           1|       1|\n",
      "|      4|           1|       1|\n",
      "|      3|           3|       3|\n",
      "|      5|           2|       2|\n",
      "+-------+------------+--------+\n",
      "\n",
      "+------------+------------+\n",
      "|AdvertiserID|        name|\n",
      "+------------+------------+\n",
      "|           3|advertiser_3|\n",
      "|           2|advertiser_2|\n",
      "|           1|advertiser_1|\n",
      "+------------+------------+\n",
      "\n",
      "+---------+-------------+\n",
      "|CompanyID|compaing_name|\n",
      "+---------+-------------+\n",
      "|        3|    company_3|\n",
      "|        1|    company_1|\n",
      "|        2|    company_2|\n",
      "+---------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM test.sales;\").show(), spark.sql(\"SELECT * FROM test.advertiser;\").show(), spark.sql(\"SELECT * FROM test.company;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|AdvertiserID|        name|\n",
      "+------------+------------+\n",
      "|           3|advertiser_3|\n",
      "|           2|advertiser_2|\n",
      "|           1|advertiser_1|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM test.advertiser;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sales = spark.sql(\"SELECT * FROM test.sales;\")\n",
    "data_advertiser = spark.sql(\"SELECT * FROM test.advertiser;\")\n",
    "data_company = spark.sql(\"SELECT * FROM test.company;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT a.AdvertiserID ,\n",
    "          a.name, count(s.SalesID),\n",
    "          count( s.compaign)\n",
    "          FROM test.advertiser as a, test.sales as s \n",
    "          where s.advertise_id=a.AdvertiserID \n",
    "          group by a.name, a.AdvertiserID;\n",
    "          \"\"\").createOrReplaceTempView(\"count_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------+---------------+\n",
      "|AdvertiserID|        name|count(SalesID)|count(compaign)|\n",
      "+------------+------------+--------------+---------------+\n",
      "|           1|advertiser_1|             4|              4|\n",
      "|           2|advertiser_2|             2|              2|\n",
      "|           3|advertiser_3|             2|              2|\n",
      "+------------+------------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM count_;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP VIEW count_;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----------+-------------+\n",
      "|AdvertiserID|name|count_sales|count_company|\n",
      "+------------+----+-----------+-------------+\n",
      "+------------+----+-----------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 15:29:35 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "24/12/12 15:29:35 WARN HiveMetaStore: Location: file:/home/alexandr/stydy/eda/sql_git/SQL/ETL/spark-warehouse/test.db/denorm_advertiser specified for non-external table:denorm_advertiser\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS test.denorm_advertiser (AdvertiserID Int,name String, count_sales Int, count_company Int);\")\n",
    "spark.sql(\"SELECT * FROM test.denorm_advertiser;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+------------+\n",
      "|ClientID|costomer_name|country_id|country_name|\n",
      "+--------+-------------+----------+------------+\n",
      "+--------+-------------+----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 15:29:36 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "24/12/12 15:29:36 WARN HiveMetaStore: Location: file:/home/alexandr/stydy/eda/sql_git/SQL/ETL/spark-warehouse/test.db/client specified for non-external table:client\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          CREATE TABLE IF NOT EXISTS test.client (\n",
    "          `ClientID` Int,\n",
    "          `costomer_name` String,\n",
    "          `country_id` Int,\n",
    "          `country_name` String\n",
    "          );\n",
    "          ''')\n",
    "spark.sql(\"SELECT * FROM test.client;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------+--------+-------------+\n",
      "|SalesID|advertise_id|        name|compaign|compaing_name|\n",
      "+-------+------------+------------+--------+-------------+\n",
      "|      1|           1|advertiser_1|       1|    company_1|\n",
      "|      2|           2|advertiser_2|       2|    company_2|\n",
      "|      8|           1|advertiser_1|       1|    company_1|\n",
      "|      6|           3|advertiser_3|       3|    company_3|\n",
      "|      7|           1|advertiser_1|       1|    company_1|\n",
      "|      4|           1|advertiser_1|       1|    company_1|\n",
      "|      3|           3|advertiser_3|       3|    company_3|\n",
      "|      5|           2|advertiser_2|       2|    company_2|\n",
      "+-------+------------+------------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT s.SalesID, s.advertise_id, a.name, s.compaign, c.compaing_name\n",
    "          FROM test.sales as s, test.advertiser as a, test.company as c\n",
    "          where s.advertise_id=a.AdvertiserID and s.compaign=c.CompanyID\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 15:29:36 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "24/12/12 15:29:36 WARN HiveMetaStore: Location: file:/home/alexandr/stydy/eda/sql_git/SQL/ETL/spark-warehouse/test.db/sales_full specified for non-external table:sales_full\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          CREATE TABLE IF NOT EXISTS test.sales_full(\n",
    "          SELECT s.SalesID, s.advertise_id, a.name, s.compaign, c.compaing_name\n",
    "          FROM test.sales as s, test.advertiser as a, test.company as c\n",
    "          where s.advertise_id=a.AdvertiserID and s.compaign=c.CompanyID\n",
    "          );\n",
    "          ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------+--------+-------------+\n",
      "|SalesID|advertise_id|        name|compaign|compaing_name|\n",
      "+-------+------------+------------+--------+-------------+\n",
      "|      8|           1|advertiser_1|       1|    company_1|\n",
      "|      3|           3|advertiser_3|       3|    company_3|\n",
      "|      4|           1|advertiser_1|       1|    company_1|\n",
      "|      5|           2|advertiser_2|       2|    company_2|\n",
      "|      2|           2|advertiser_2|       2|    company_2|\n",
      "|      6|           3|advertiser_3|       3|    company_3|\n",
      "|      1|           1|advertiser_1|       1|    company_1|\n",
      "|      7|           1|advertiser_1|       1|    company_1|\n",
      "+-------+------------+------------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM test.sales_full;').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 15:29:37 WARN TxnHandler: Cannot perform cleanup since metastore table does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP DATABASE IF EXISTS test CASCADE;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
